\documentclass{article}
\usepackage[utf8]{inputenc}

\input{preamble}

\begin{document}

\section{Introduction}

BIG TODO: Write something about Pure Type Systems and the lambda cube and how
this language is essentially an extension of the Calculus of Constructions, with
stuff like top level definitions and lightweight Typed Source esque type
inference.

Our language is a dependently typed lambda calculus, based on the calculus of
constructions.
The main references which we base our language on are
\href{https://www.andres-loeh.de/LambdaPi/LambdaPi.pdf}{this paper on $\lambda
  \Pi$} and \href{https://github.com/andrejbauer/spartan-type-theory}{this
  implementation of a dependent type theory}.

In System F, types may depend on other type variables, thus enabling
parametrically polymorphic functions. 
In our language, we generalize this to allow types to depend on terms as well.
In order to achieve this, we promote all types to expressions, so we no longer
have separate syntactic categories for them. 

Yes, this means that \textit{everything} in our language is an expression, even those
things which we call types!
With this in mind, a type is then an expression, say $T$, which satisfies a very
specific typing judgment, ie $\Gamma \vdash T \Leftarrow \Type$. We'll revisit
this again later.

The main aim of our project is to implement a simple lambda calculus with the
dependent function type. These are also known as Pi types in the literature.
These generalize the simple function type $A \to B$ by allowing the output type
$B$ to now \textit{depend on the value} of the input expression.
The type of functions in our language is now written as $\Pi_{x : A} B(x)$ where
we write $B(x)$ for the return type to emphasize that $x$ may appear free in
$B$.

The set theoretic analogue to this dependent Pi type is the generalized
cartesian product. Given a set $A$ and a family of sets $\langle B_x \, | \, x
\in A\rangle$ indexed by the
elements $x \in A$, we can form the generalized cartesian product, denoted
\begin{align*}
  \Pi_{x \in A} B_x = \bigg\{ f : A \rightarrow \bigcup_{x \in A} B_x \, | \, \forall x \in A, \, f(x) \in B_x \bigg\}
\end{align*}
Functions that inhabit this set are known as \textit{choice functions} in set
theory. Such choice functions associate to each $x \in A$, an element $f(x)$ in $B_x$.
As a fun fact, the Axiom of Choice asserts that this set is nonempty
if every $B_x$ is inhabited.

Another core feature of our language will be type inference.
This will be implemented alongside typechecking, using a fancy
\textit{bidirectional typechecking} algorithm. Don't worry, we'll formalize all
this later. For now, this just means that typechecking and type inference are
mutually recursive processes.

Our language also doesn't have full blown recursion and is
\textit{strongly normalizing} in that every sequence of beta reductions will
always terminate in a unique head normal form. This allows us to safely
normalize all terms to full head normal form.

TODO: explain why our language is strongly normalizing and why this allows us to
normalize everything all the way.

\section{Syntax}
Here we describe the concrete ascii syntax for our language.
We also describe the syntactic sugar users can enter. This includes unicode
symbols.

Note that we have separate syntax for expressions and statements, the latter of 
which function like top-level commands. These will be used by the user to interact
with our language.

\subsection{Syntax for expressions}
We define the set of valid expressions of our language as the initial algebra, 
ie least prefixed point, of the endofunctor
\begin{align*}
  F : X \mapsto \{ E \, | \, \{ E_0, E_1, \dots, E_n \} \subseteq X \text{ and }
    \begin{prooftree}   
      \hypo{E_0}
      \hypo{E_1}
      \hypo{\dots}
      \hypo{E_n}
      \infer4{E}
    \end{prooftree}
    \text{ is a rule instance}
  \} 
\end{align*}
on some complete powerset lattice.
Such a construction can be justified by the famous fixed point theorems of Tarski
and Kleene.

Before we list the full set of rules, we first introduce some metavariables.

\subsubsection{Metavariables}
\begin{enumerate}
  \item $A$, $B$, $T$, $E$ range over expressions. 
  \item $x$, $y$, $z$ range over variables.
\end{enumerate}

\subsubsection{Syntax rules}
\begin{enumerate}
\item \textbf{Sorts} \\
  \[
    \begin{prooftree}
      \infer0{\Type}
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer0{\Kind}
    \end{prooftree}
  \]

  Our language has 2 sorts, just like the original CoC. In the spirit of Curry
  Howard, our language makes no distinction between types and propositions and
  so users can enter \verb|Prop| (short for ``proposition") in place of \verb|Type|.

  This syntactix sugar has the unfortunate consequence of conflating the syntactic
  roles of propositions and types.
  Types which we think of as data can be used interchangeably with types which
  we view as propositions.
  
  This may be counter-intuitive for users because in first (or higher) order
  logic, there is a clear distinction between terms and well-formed formulae,
  which belong to different syntactic categories.

\item \textbf{Variables} \\
  \[
    \begin{prooftree}
      \infer0{x}
    \end{prooftree}
  \]

\item \textbf{Optional parentheses} \\
  \[
    \begin{prooftree}
      \hypo{E}
      \infer1{( \, E \, )}
    \end{prooftree}
  \]
  
\item \textbf{Function abstraction} \\
  \[
    \begin{prooftree}
      \hypo{x}
      \hypo{E}
      \infer2{\fun \, x => E}
    \end{prooftree}
  \]
  \[
    \begin{prooftree}
      \hypo{x_0}
      \hypo{x_1}
      \hypo{\dots}
      \hypo{x_n}
      \hypo{E}
      \infer5{\fun \, x_0 \,\, x_1 \, \dots \, x_n => E}
    \end{prooftree}
  \]
  Note that all functions in our language are unary and so
  $\fun \, x_0 \, x_1 \, \dots \, x_n => E$ is syntactic sugar for
  \[ \fun \, x_0 => (\fun \, x_1 => \dots (\fun x_n => E)) \]

  Simimarly, one can also provide optional type annotations for input variables.
  This is to help the type checker infer the type of a function.
  \[
    \begin{prooftree}
      \hypo{x}
      \hypo{T}
      \hypo{E}
      \infer3{\fun \, (x : T) \, => E}
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{x_i}
      \hypo{T_i}
      \hypo{E}
      \infer3{\fun \, (x_0 : T_0) \,\, (x_1 : T_1) \, \dots \, (x_n : T_n) => E}
    \end{prooftree}
  \]
  We also treat $\fun \, (x_0 : T_0) \,\, (x_1 : T_1) \, \dots \, (x_n : T_n) => E$
  as syntactic sugar for
  \[ \fun \, (x_0 : T_0) => (\fun \, (x_1 : T_1) => \dots (\fun (x_n : T_n) => E)) \]

  As syntactic sugar, we allow users to use \verb|lambda| and $\lambda$ in place
  of \verb|fun|. 

\item \textbf{Pi and Sigma type} \\
  \[
    \begin{prooftree}
      \hypo{x}
      \hypo{A}
      \hypo{B}
      \infer3{\Pii \, (x : A), \, B}
    \end{prooftree}
  \] 

  \[
    \begin{prooftree}
      \hypo{x_i}
      \hypo{A_i}
      \hypo{B}
      \infer3{\Pii \, (x_0 : A_0) \, (x_1 : A_1) \, \dots \, (x_n : A_n), \, B}
    \end{prooftree}
  \] 
  As with functions, this is syntactic sugar for
  \[ \Pii \, (x_0 : A_0), \, (\Pii \, (x_1 : A_1), \,  \dots \, (\Pii \, (x_n : A_n), \, B)) \]

  If there is only one pair of input type and type annotation following the \verb|Pi|,
  the brackets may be ommitted so users can enter \verb|Pi x : A, B| instead of
  \verb|Pi (x : A), B|.

  The syntax rules for Sigma, ie $\Sigmaa\, (x : A), B$ and the above syntactic
  sugar work the same as with Pi above. 

  We also allow users to enter, in place of \verb|Pi|, \verb|forall| or using
  unicode, $\forall$ and $\Pi$.
  Similarly, users can enter \verb|exists|, $\Sigma$ and $\exists$ instead of
  \verb|Sigma|.

  In the event that the output type does not depend on the input type, users
  may enter \verb|A -> B| and \verb|A * B| in place of 
  \verb|Pi (x : A), B| and \verb|Sigma (x : A), B|.

  \item \textbf{Sigma constructor} \\
  \[
    \begin{prooftree}
      \hypo{E_1}
      \hypo{E_2}
      \infer2{(E_1, \, E_2)}
    \end{prooftree}  
  \]

  \item \textbf{Sigma eliminators} \\
  \[
    \begin{prooftree}
      \hypo{E}
      \infer1{\fst \, E}
    \end{prooftree}
  \]

  \[
   \begin{prooftree}
    \hypo{E}
    \infer1{\snd \, E}
   \end{prooftree}
 \]

\item \textbf{Type ascriptions} \\
  \[
    \begin{prooftree}
      \hypo{E}
      \hypo{T}
      \infer2{(E : T)}
    \end{prooftree}
  \]
  This functions similarly to other functional languages in that it's used
  mainly to provide type annotations. For instance, it can be used to help the
  type checker if it's unable to infer the type of an expression.

\item \textbf{Local let bindings} \\
\[
  \begin{prooftree}
    \hypo{x}
    \hypo{E}
    \hypo{E'}
    \infer3{\text{let } x := E \text{ in } E'}
  \end{prooftree}
\]

\item \textbf{Sum type} \\
\[
  \begin{prooftree}
    \hypo{A}
    \hypo{B}
    \infer2{A + B}
  \end{prooftree}
\]

\item \textbf{Sum type constructor} \\
\[
  \begin{prooftree}
    \hypo{E}
    \infer1{\inl E}
  \end{prooftree}
\]

\[
  \begin{prooftree}
    \hypo{E}
    \infer1{\inr E}
  \end{prooftree}
\]

These are meant for introducing the left and right components of a disjoint sum,
ie $\inl E$ constructs an expression of type $A + B$ given an expression $E$ of
type $A$. Similarly, $\inr E$ constructs an $A + B$ given $E$ of type $B$.

\item \textbf{Sum type eliminator} \\
Given expressions $E$ and variables $x$ and $y$, users can perform case analysis
on a sum type via the \verb|match| construct below

\begin{verbatim}
  match E with
  | inl x -> E1
  | inr y -> E2
  end
\end{verbatim}

The ordering of both clauses can be swapped so users can also enter
\begin{verbatim}
  match E with
  | inr y -> E2
  | inl x -> E1
  end
\end{verbatim}

Note here that $x$ is bound in the expression that is $E_1$, while $y$ is bound
in $E_2$.

This \verb|match| construct is fashioned after the similarly named pattern
matching construct in ML like languages.
However, unlike those, we do not implement actual pattern matching since pattern
matching for dependent types is not an easy problem.
Our \verb|match| construct serves the sole purpose of allowing one to perform
case analysis on sum.

For convenience, we write 
$\match(E, \, x \rightarrow E_1, \, y \rightarrow E_2)$ to denote this
construct.

\end{enumerate}

\subsubsection{Syntactic sugar: wildcard variables}
In the event where the user does not intend to use the variable being bound,
like say in a lambda expression, the underscore, \verb|_|, can be used in place
of a variable name.
For instance, one can enter \verb|fun _ => Type| in place of \verb|fun x => Type|.
As in other languages, \verb|_| is not a valid identifier that can be used
anywhere else in an expression. It can only be used in place of a variable in a
binder.

\subsubsection{A word about notation}
Note that we often write $\lambda x, \, E$ instead of $\fun \, x => E$ as in the
concrete syntax. Similarly, we also write $\lambda (x : T), \, E$ in place of
$\fun \, (x : T) => x$.
Finally, we use $\Pi_{x : A}B(x)$ to abbreviate $\Pii \, (x : A), \, B$.

\subsection{Syntax for statements}
Statements are top level commands through which users interact with our
language. Programs in our language will be \textit{nonempty} sequences of
these statements.

\begin{enumerate}
\item \textbf{Def} \\
\[
  \begin{prooftree}
    \hypo{x}
    \hypo{E}
    \infer2{\deff \, x := E}
  \end{prooftree}
\]
This creates a top level, global definition, binding the variable $x$ to the
expression given by $E$.

\item \textbf{Axiom} \\
\[
  \begin{prooftree}
    \hypo{x}
    \hypo{T}
    \infer2{\axiom \, x : T}
  \end{prooftree}
\]
This defines the variable $x$ to have a type of $T$ with an unknown binding.
The interpreter will treat $x$ like an unknown, indeterminate constant.

We also allow users to enter \verb|constant x : T| instead of \verb|axiom x : T|,
as syntactic sugar. Note that since our system conflates the notion of types and
propositions, introducing a constant \verb|x| of type \verb|T| is akin to
introducing an assumption (ie axiom).

\item \textbf{Check} \\
\[
  \begin{prooftree}
    \hypo{E}
    \infer1{\checkk \,\, E}
  \end{prooftree}
\]
This instructs the interpreter to compute the type of the expression $E$ and
output it.

\item \textbf{Eval} \\
\[
  \begin{prooftree}
    \hypo{E}
    \infer1{\evall \,\, E}
  \end{prooftree}
\]
This instructs the interpreter to fully normalize the expression $E$.

In the next section, we will properly define the notion of normalization using a
big step semantics. For now it suffices to say that normalizing an expression
is the process of fully evaluating it until no more simplifications can be
performed.

\end{enumerate}

\section{Capture avoiding substitutions}
Here we define the notion of capture avoiding substitutions. For this, we
formalize the notion of free variables and substitution.

\subsection{Free variables}
We define the set of free variables of an expression $E$, ie $\FV(E)$ recursively.
\begin{align*}
  \FV (x) &:= \{x\} \\
  \FV (E_1 \, E_2) &:= \FV(E_1) \cup \FV(E_2) \\
  \FV (\lambda \, x, \, E(x)) &:= \FV(E) \setminus \{x\} \\
  \FV (\lambda \, (x : T), \, E(x)) &:= \FV(T) \cup (\FV(E) \setminus \{x\}) \\
  \FV (\text{let } x := E \text{ in } E') &:= \FV((\lambda x, \, E') \,\, E) \\
  \FV (\Pi_{x : A} B(x)) &:= \FV(A) \cup (\FV(B) \setminus \{x\})
\end{align*}
Note that in Pi expressions, the input type $A$ is actually an expression itself
and so may contain free variables. It's important to note that the $\Pi_{x :
  A}$, like a lambda is a binder binding $x$ in $B$. However, this does not bind
$x$ in the input type, $A$. If $x$ does appear in the input type $A$, it is
free there.

\begin{align*}
 \FV (\Sigma_{x : A} B(x) &:= \FV(\Pi_{x : A} B(x))
\end{align*}

Sigma expressions follow the same rules as with Pi expressions, with $x$ being
bound in the output type $B$ but not in the input type $A$.

\begin{align*}
  \FV (A + B) &:= \FV(A) \cup \FV(B) \\
  \FV (\match(E, x \rightarrow E_1, y \rightarrow E_2)) &:= 
    E \cup (E_1 \setminus \{x\}) \cup (E_2 \setminus \{y\}))
\end{align*}
Note that in match expressions, $x$ is bound in $E_1$ and $y$ is bound in $E_2$.

For the expressions $\fst \, E$, $\snd \, E$, $\inl \, E$ and $\inr \, E$, the
set of free variables is precisely $\FV(E)$, since those keywords are treated
like constants.

\subsection{Substitution}
Here we define $E [x \mapsto E'] := E''$ to mean substituting all free
occurrences of $x$ by $E'$ in the expression $E$ yields another expression $E''$.
\begin{align*}
  x[x \mapsto E] &:= E \\
  y[x \mapsto E] &:= y \quad (\text{if } y \ne x) \\
  (E_1 \,\,\, E_2) [x \mapsto E] &:= (E_1 [x \mapsto E] \,\,\, E_2 [x \mapsto E]) \\
  (\lambda x, \, E) [x \mapsto E'] &:= \lambda x, \, E \\ 
  (\lambda y, \, E) [x \mapsto E'] &:= \lambda y, \, E[x \mapsto E'] \quad (\text{if } y \notin \FV(E')) \\ 
  (\lambda y, \, E) [x \mapsto E'] &:= \lambda z, E[y \mapsto z][x \mapsto E'] \\
                 &\quad \quad \quad (\text{if } z \notin FV(E) \cup FV(E') \cup \{x\}) \\
  (\text{let } y := E \text{ in } E'') [x \mapsto E'] &:= 
    ((\lambda y, \, E) \,\, E'') [x \mapsto E']
\end{align*}

We also define $(\lambda \, (x : T), \, E) [x \mapsto E']$
similar to the case without the optional type annotation above. The only
difference here is we also need to substitute $x$ for $E'$ in the type annotation,
$T$. Note that we do not treat $x$ as being bound in $T$ here.

\begin{align*}
  (\Pi_{x : A} B(x)) [x \mapsto B'] &:= \Pi_{x : A[x \mapsto B']} B(x) \\ 
  (\Pi_{y : A} B(y)) [x \mapsto B'] &:= \Pi_{y : A[x \mapsto B']} B[x \mapsto B'] \quad (\text{if } y \notin \FV(B')) \\ 
  (\Pi_{y : A} B(y)) [x \mapsto B'] &:= \Pi_{z : A[x \mapsto B']} B[y \mapsto z][x \mapsto B'] \\
                 &\quad \quad \quad (\text{if } z \notin FV(B) \cup FV(B') \cup \{x\})
\end{align*}

For Pi expressions, $A$ is an expression and thus when substituting $x$ by
$B'$, we must also perform the substitution in the input type $A$. However, as
$x$ is not bound there, we need not worry about capturing it when substituting there. 

For Sigma expressions, we define $(\Sigma_{x : A} B(x)) [x \mapsto B']$ in a
similar fashion as with the case of Pi.

The substitution rules for \verb|fst|, \verb|snd|, \verb|inl| and \verb|inr| are
trivial and thus ommitted.

Finally, we avoid specifying substitution formally for match expressions since it's
tedious. The key thing to note is that in the expression
$\match(E, x \rightarrow E_1, y \rightarrow E_2)$, $x$ is bound in $E_1$ and $y$
is bound in $E_2$.

\section{Big step operational semantics}
\begin{comment}
  https://www.andres-loeh.de/LambdaPi/LambdaPi.pdf
  http://math.andrej.com/2012/11/08/how-to-implement-dependent-type-theory-i/
  
  http://fsl.cs.illinois.edu/images/archive/b/b3/20110221180817!CS522-Spring-2011-PL-book-bigstep.pdf
  https://www.cs.cornell.edu/courses/cs4110/2010fa/lectures/lecture03.pdf
\end{comment}

In this section, we define a big-step semantics for our language.
We follow the approach of Plotkin, as described in his
\href{https://web.eecs.umich.edu/~weimerw/2014-6610/reading/plotkin81structural.pdf}
{famous technical report} on structural operational semantics.

\subsection{Expressions}
\subsubsection{Head normal form, neutral expressions, beta equivalence}
\textbf{Head normal form and neutral expressions}

In the simply typed lambda calculus in which there is only the simple function 
type and lambda abstraction, normalization is the process in which 
expressions are simplified through repeated applications of the beta reduction rule
into a form in which no further beta reductions can occur anywhere in the 
expression, even underneath the outermost lambda.
The resulting expression is said to be in \textit{head normal form}. In this
section, we want to define similar notions for our richer language.

First, note that we have more than just function types in our language. In
particular, we have 3 main types, namely \verb|Pi|, \verb|Sigma| and \verb|Sum|.
Also, ``types'' themselves are expressions too.

Next note that we will be performing normalization and substitution with
respect to a context, which contains definitions which the user declares
globally, via \verb|def| and \verb|axiom| statements.

Since \verb|axiom| statements introduce variables at the global level with a
type but no binding, expressions can now contain free variables.
Hence we need a definition of head normal form and normalization
that takes care of expressions with free variables.
For this, we must introduce the concept of a \textit{neutral expression}.

Neutral expressions are those which cannot be reduced via one of the elimination
rules (ie the ones for \verb|Pi|, \verb|Sigma| and \verb|Sum| types) because 
the expression to be eliminated is a free variable rather than an expression of
the appropriate type.

With this, we generalize the notion of head normal form to be one in which
the expression cannot be further simplified by applying any of the 3
elimination rules corresponding to the aforementioned types.

Normalization is then the process in which we repeatedly simplify an expression
using these elimination rules, with the aim of reducing it to a head normal form.
In the rules to follow, we shall see that this head normal form of an expression,
should it exist, is unique, just as in the simply typed lambda calculus.

\begin{definition} [Neutral expressions and head normal form]
We now define, using mutual recursion, the subset of neutral and 
normalized expressions via the following judgments:
\begin{enumerate}
  \item $\hnf(E)$ \\
    This asserts that $E$ is an expression that is in head normal form.
  
  \item $\neutral(E)$ \\
    This asserts that $E$ is a neutral expression.
\end{enumerate}

The rules are as follows:
\begin{enumerate}
  \item \[
    \begin{prooftree}
      \hypo{\neutral(E)}
      \infer1{\hnf(E)}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \hypo{\hnf(E)}
      \infer1{\hnf(\lambda x, \, E)}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \hypo{\hnf(E_1)}
      \hypo{\hnf(E_2)}
      \infer2{\hnf(\Pi_{x : E_1}, \, E_2)}
    \end{prooftree}
  \]

 \item \[
    \begin{prooftree}
      \hypo{\hnf(E_1)}
      \hypo{\hnf(E_2)}
      \infer2{\hnf(\Sigma_{x : E_1}, \, E_2)}
    \end{prooftree}
  \]

 \item \[
    \begin{prooftree}
      \hypo{\hnf(E_1)}
      \hypo{\hnf(E_2)}
      \infer2{\hnf(E_1 + E_2)}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \infer0{\neutral(x)}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \hypo{\neutral(E_1)} 
      \hypo{\hnf(E_1)} 
      \infer2{\neutral(E_1 \,\, E_2)}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \hypo{\neutral(E)} 
      \hypo{\hnf(E_1)} 
      \hypo{\hnf(E_2)} 
      \infer3{\neutral(\match(E, \, x \rightarrow E_1, \, y \rightarrow E_2))}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \hypo{\neutral(E)} 
      \infer1{\neutral(\fst \, E)}
    \end{prooftree}
  \]

  \item \[
    \begin{prooftree}
      \hypo{\neutral(E)} 
      \infer1{\neutral(\snd \, E)}
    \end{prooftree}
  \]
\end{enumerate}
\end{definition}

Note that \verb|let ... in ...| expressions are not considered to be in head normal form
because we treat expressions of the form \verb|let x := E in E'| as syntactic
sugar for the application $(\lambda x, E') \, E$ during the process of
normalization. 

In fact, we will see later that the typing rules for \verb|let|
expressions are also derived using this syntactic trick. 

Finally, we also define the notion of beta equivalence. 

\begin{definition} [Beta equivalence]
We say that two expressions are beta equivalent to each other, written
$E_1 \equiv_\beta E_2$ if they have the same head normal form.

If both expressions are also alpha equivalent, ie
they're equal up to renaming of bound variables, we write
$E_1 \equiv_{\alpha \beta} E_2$.
\end{definition}

\subsubsection{Metavariables for neutral and normalized expressions}
We now introduce 2 more types of metavariables corresponding to the new definitions
given in the previous section. 

\begin{enumerate}
\item $\nu, \, \tau$ range over normalized expressions, ie those in head normal
form.
\item $n$ ranges over all neutral terms.
\end{enumerate}

\subsubsection{Contexts}
Here we take the context, $\Gamma$, to be a list, ie a finite sequence, of triples
of the form
\begin{align*}
  (\text{variable name}, \, \text{type of variable}, \, \text{binding})
\end{align*}
We will use $\emptyset$ to denote the empty context, and $::$ to refer to the
list cons operation.

The binding can be an expression or a special undefined value, which we denote by
\verb|und|.
Contexts have global scope and the top level statements \verb|def|
and \verb|axiom|, return new contexts with updated bindings.
The special \verb|und| value is used for the bindings created by \verb|axiom|
statements and when we want to add a binding for type checking purposes.
In such scenarios, we don't particularly care about the actual binding. We're
only interested in the type of the variable. 

We should mention that whenever we write, say
$(x, \, \tau, \, \nu) \in \Gamma$, we allow the metavariable $\nu$ to be 
\verb|und| as well. Also if there are multiple occurrences of $x$ in $\Gamma$,
as is the case when there is variable shadowing, we refer to the first occurrence
of $x$. 

\subsubsection{Overview of judgement forms}
We first give an overview of the main judgment forms which we will define in a
mutually recursive fashion in the subsequent few sections.

\begin{enumerate}
\item $\wf(\Gamma)$ \\
  This asserts that a context $\Gamma$ is well formed.

\item $\Gamma \vdash E \Leftarrow T$ and $\Gamma \vdash E \Rightarrow T$ \\
  These judgments will be used to formalize our bidirectional typechecking
  algorithm. They form the typing rules for our language.
  For now, it suffices to say that $\Gamma \vdash E \Leftarrow T$ formalizes the
  meaning that given an expression $E$ and some type $T$, we may verify that
  $E$ has type $T$ under the context $\Gamma$.

  On the other hand, $\Gamma \vdash E \Rightarrow T$ formalizes the notion of
  \textit{type inference}. It says that from a context $\Gamma$, we may infer
  the type of $E$ to be $T$. 
  
  It should be noted here that this isn't real type
  inference using constraint solving and unification. It's a form of lightweight
  type inference that can infer simple stuff like the return type of a function
  application but not the type parameter in the polymorphic identity function
  $\lambda \, (T : \Type) (x : T), \, x$.

\item $\Gamma \vdash E \Downarrow \nu$ \\
  This $\cdot \vdash \cdot \Downarrow \cdot$ relation is used in our definition of a
  big-step operational semantics to normalize expressions to their full head
  normal form.
  It says that with respect to a context $\Gamma$ containing global bindings, we
  may normalize $E$ to an expression, $\nu$ that is in head normal form.
  % Note that we can do so because our language is a lambda calculus that does
  % not contain full blown recursion and is strongly normalizing. 
\end{enumerate}

\subsubsection{Big step normalization}
Before defining our type system given by the 2 judgments,
$\cdot \vdash \cdot \Leftarrow \cdot$ and $\cdot \vdash \cdot \Rightarrow
\cdot$, we first define $\cdot \vdash \cdot \Downarrow \cdot$, the big
step semantics for normalizing expressions. This is because we will actually
need to perform normalization while typechecking.

Recall that the aim of normalization is to reduce an expression to head normal
form, which includes neutral expressions and free variables as a subset.

The rules are as follows:

% It should be noted at this point that in our semantics to follow, we only 
% normalize terms after we typecheck them. 
% In the case of expressions that denote types, this means that we always check
% that they are well formed before performing any computations on them.

% Since \verb|Kind| has no type, we shall not need to define what it means to
% normalize it. 

\begin{enumerate}
\item \textbf{Type}
  \[
    \begin{prooftree}
      \hypo{\wf(\Gamma)}
      \infer1{\Gamma \vdash \Type \Downarrow \Type} 
    \end{prooftree}
  \]
\item \textbf{Variables} \\
  \[
    \begin{prooftree}
      \hypo{\wf(\Gamma)}
      \hypo{(x, \, \tau, \, \nu) \in \Gamma}
      \infer2{\Gamma \vdash x \Downarrow \nu}
    \end{prooftree} 
  \]

  \[
    \begin{prooftree}
      \hypo{\wf(\Gamma)}
      \hypo{(x, \, \tau, \, \text{und}) \in \Gamma}
      \infer2{\Gamma \vdash x \Downarrow x}
    \end{prooftree} 
  \]

\item \textbf{Type ascriptions} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Downarrow \nu}
      \infer1{\Gamma \vdash (E : T) \Downarrow \nu}
    \end{prooftree}
  \]
  This says type ascriptions do not play a role in computation. They're just
  there to help the type checker figure things out and for users to communicate
  their intentions.
 
\item \textbf{Pi elimination, ie function application} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E_1 \Downarrow \lambda x, \, \nu_1}
      \hypo{\Gamma \vdash E_2 \Downarrow \nu_2}
      \hypo{\Gamma \vdash \nu_1[x \mapsto \nu_2] \Downarrow \nu}
      \infer3{\Gamma \vdash E_1 \,\, E_2 \Downarrow \nu}
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E_1 \Downarrow n}
      \hypo{\Gamma \vdash E_2 \Downarrow \nu}
      \infer2{\Gamma \vdash E_1 \,\, E_2 \Downarrow n \,\, \nu}
    \end{prooftree}
  \]

  The last rule handle the cases of neutral expressions.

\item \textbf{Normalizing under lambdas} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Downarrow \nu}
      \infer1{\Gamma \vdash \lambda x, \, E, \, \Downarrow \lambda x, \, \nu}
    \end{prooftree}
  \]
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash \lambda x, \, E \Downarrow \nu}
      \infer1{\Gamma \vdash \lambda \, (x : T), \, E, \, \Downarrow \nu}
    \end{prooftree}
  \]
  This second rule says that optional type ascriptions do not play a role in
  normalization, ie we just ignore them.

\item \textbf{Pi type constructor} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash A \Downarrow \tau}
      \hypo{\Gamma \vdash B(x) \Downarrow \tau'(x)}
      \infer2{\Gamma \vdash \Pi_{x : A} B(x) \Downarrow \Pi_{x : \tau} \tau'(x)}
    \end{prooftree}
  \]

\item \textbf{Local let binding} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E \Downarrow \nu}
        \hypo{\Gamma \vdash E'[x \mapsto \nu] \Downarrow \nu'}
        \infer2{\Gamma \vdash \text{let } x := E \text{ in } E' \Downarrow \nu'}
      \end{prooftree}
    \]

\item \textbf{Normalizing under pair constructor} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E_1 \Downarrow \nu_1}
        \hypo{\Gamma \vdash E_2 \Downarrow \nu_2}
        \infer2{\Gamma \vdash \langle E_1, \, E_2 \rangle \Downarrow \langle
          \nu_1, \, \nu_2 \rangle}
      \end{prooftree}
    \]

  \item \textbf{Sigma type constructor} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash A \Downarrow \tau}
        \hypo{\Gamma \vdash B \Downarrow \tau'(x)}
        \infer2{\Gamma \vdash \Sigma_{x : A}B(x) \Downarrow \Sigma_{x : \tau}\tau'(x)}
      \end{prooftree}
    \]

  \item \textbf{Sigma elimination} \\
   \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Downarrow (\nu_1, \, \nu_2)}
      \infer1{\Gamma \vdash \fst E \Downarrow \nu_1}
    \end{prooftree}
  \]

  \[
   \begin{prooftree}
    \hypo{\Gamma \vdash E \Downarrow n}
     \infer1{\Gamma \vdash \fst E \Downarrow \fst n}
   \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Downarrow (\nu_1, \, \nu_2)}
      \infer1{\Gamma \vdash \snd E \Downarrow \nu_2}
    \end{prooftree}
  \]

  \[
   \begin{prooftree}
     \hypo{\Gamma \vdash E \Downarrow n}
     \infer1{\Gamma \vdash \snd E \Downarrow \snd n}
   \end{prooftree}
  \]

  % \[
  %   \begin{prooftree}
  %     \hypo{\Gamma \vdash E \Downarrow \langle \nu_1, \, \nu_2 \rangle}
  %     \hypo{\Gamma \vdash E'[x_1 \mapsto \nu_1][x_2 \mapsto \nu_2] \Downarrow \nu}
  %     \infer2{\Gamma \vdash \text{let } (x_1, \, x_2) := E \text{ in } E' \Downarrow \nu}
  %   \end{prooftree}
  % \]

  \item \textbf{Sum type constructor}
  \[
    \begin{prooftree}
      \hypo{E_1 \Downarrow \nu_1}
      \hypo{E_2 \Downarrow \nu_2}
      \infer2{E_1 + E_2 \Downarrow \nu_1 + \nu_2}
    \end{prooftree}
  \]

  \item \textbf{Normalizing under sum data constructors}
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Downarrow \nu}
      \infer1{\Gamma \vdash \inl \, E \Downarrow \inl \, \nu}
    \end{prooftree}  
  \]

  \[
    \begin{prooftree}
     \hypo{\Gamma \vdash E \Downarrow \inl \nu}
     \infer1{\Gamma \vdash \inr \, E \Downarrow \nu}
    \end{prooftree}  
  \]

  \item \textbf{Normalizing sum eliminator}
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Downarrow \inl \nu}
      \hypo{\Gamma \vdash E_1[x \mapsto \nu] \Downarrow \nu_1}
      \infer2{\Gamma \vdash 
        \match(E, \, x \rightarrow E_1, \, y \rightarrow E_2)
        \Downarrow \nu_1}
    \end{prooftree}  
  \]

  \[
   \begin{prooftree}
     \hypo{\Gamma \vdash E \Downarrow \inr \nu}
     \hypo{\Gamma \vdash E_2[y \mapsto \nu] \Downarrow \nu_2}
     \infer2{\Gamma \vdash 
       \match(E, \, x \rightarrow E_1, \, y \rightarrow E_2)
       \Downarrow E_2[y \mapsto \nu_2]}
   \end{prooftree}  
  \]

  \[
   \begin{prooftree}
    \hypo{\Gamma \vdash E \Downarrow n}
      \hypo{\Gamma \vdash E_1 \Downarrow \nu_1}
      \hypo{\Gamma \vdash E_2 \Downarrow \nu_2}
      \infer3{\Gamma \vdash \match(E, \, x \rightarrow E_1, \, y \rightarrow E_2) 
               \Downarrow
               \match(n, \, x \rightarrow \nu_1, \, y \rightarrow \nu_2) 
               }
   \end{prooftree}  
  \]

\end{enumerate}

Note that the lack of a normalization rule for \verb|Kind| is deliberate.
The reason is that we will ensure that we only normalize expressions after we
typecheck them, and we will see in the next section that \verb|Kind| has no
type. Thus we will never need to normalize it.

\subsubsection{Well formed context and typing judgments}
\begin{definition} [Well formed type and type constructor]
  An expression $T$ is said to be a well-formed type with respect to the context
  $\Gamma$ if it satisfies
  \[ \Gamma \vdash T \Leftarrow s \]

  Informally, we can think of $\Type$ as the ``type of all small types'' and so
  all small well formed types are those expressions satisfying
  $\Gamma \vdash T \Leftarrow \Type$, ie they can be checked to have type $\Type$.

  Those $T$ satisfying $\Gamma \vdash T \Leftarrow \Kind$ instead represent the
  type of type constructors. In other words, the type of a type constructor is a
  Kind, just like in Haskell. 
\end{definition}

\begin{definition} [Well formed context]
  Note that our definition below implies that expressions and types that we
  store in our context are normalized to head normal form.

  \begin{enumerate}
  \item \textbf{Base case} \\
    \[
      \begin{prooftree}
        \hypo{\wf(\emptyset)}
      \end{prooftree}
    \]

  \item \textbf{Inductive cases} \\
    \[
      \begin{prooftree}
        \hypo{\wf(\Gamma)}
        \hypo{\Gamma \vdash \tau \Rightarrow s}
        \infer2{\wf ((x, \, \tau, \, \nu) :: \Gamma)}
      \end{prooftree}
    \]

    \[
      \begin{prooftree}
        \hypo{\wf(\Gamma)}
        \hypo{\Gamma \vdash \tau \Rightarrow s}
        \infer2{\wf ((x, \, \tau, \, \text{und}) :: \Gamma)}
      \end{prooftree}
    \]
  \end{enumerate}
\end{definition}

\begin{definition} [Bidrectional typechecking]
  Here we define the 2 \textit{mutually recursive} relations
  \begin{enumerate}
  \item$\cdot \vdash \cdot \Rightarrow \cdot $ which corresponds to \textit{inference}
  \item$\cdot \vdash \cdot \Leftarrow \cdot$ which corresponds to \textit{checking}
  \end{enumerate}

  The idea is that there are some expressions for which it is easier to
  \textit{infer},
  ie compute the type directly, while for others, it is easier to have the user
  supply a type annotation and then \textit{check} that it is correct.

  As a rule of thumb, it is often easier to check the type for
  introduction rules while for elimination rules, it is usually easier to infer
  the type.

  \begin{enumerate}
  \item \textbf{Var} \\
    \[
      \begin{prooftree}
        \hypo{\wf(\Gamma)}
        \hypo{(x, \, \tau, \, v) \in \Gamma}
        \infer2{\Gamma \vdash x \Rightarrow \tau}
      \end{prooftree}
    \]
    This says that we may infer the type of a variable if the type information
    is already in our context.
    Note that in this rule, we also allow $\nu$ to be \verb|und|.

  \item \textbf{Type} \\
    \[
      \begin{prooftree}
        \hypo{\wf(\Gamma)}
        \infer1{\Gamma \vdash \Type \Rightarrow \Kind}
      \end{prooftree}
    \]

    % \[
    %   \begin{prooftree}
    %     \hypo{\wf(\Gamma)}
    %     \infer1{\Gamma \vdash \Type \Rightarrow \Type}
    %   \end{prooftree}
    % \]

  \item \textbf{Type ascriptions} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash T \Rightarrow s}
        \hypo{\Gamma \vdash T \Downarrow \tau}
        \hypo{\Gamma \vdash E \Leftarrow \tau}
        \infer3{\Gamma \vdash (E : T) \Rightarrow \tau}
      \end{prooftree}
    \]
    Optional type ascriptions allow the interpreter to infer the type of an
    expression. This is useful for lambda abstractions in particular because it's
    kinda hard to infer the type of a function like $\lambda x, \, x$ without any
    further contextual information.

    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E \Leftarrow \Kind}
        \infer1{\Gamma \vdash (E : \Kind) \Rightarrow \Kind}
      \end{prooftree}
    \]
    Note that the first rule doesn't allow users to assert that $(E : \Kind)$
    since there is no $s$ with $\Gamma \vdash \Kind \Rightarrow s$.
    This second rule allows users to assert that \verb|Type| and type constructors
    have type \verb|Kind|.
    
  \item \textbf{Check} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E \Rightarrow \tau'}
        \hypo{\tau \equiv_{\alpha \beta} \tau'}
        \infer2{\Gamma \vdash E \Leftarrow \tau}
      \end{prooftree}
    \]

    This says that to check if $E$ has type $\tau$ with respect to a context
    $\Gamma$, we may first infer the type of $E$. Suppose it is $\tau'$. Then
    if we also find that $\tau$ and $\tau'$ are $\alpha$ and $\beta$ equivalent
    to each other, we may conclude that $E$ indeed has type $\tau$.

    \begin{remark}
      Note that this rule, together with the one on type ascriptions, is the reason
      why we need to perform computations on types, ie normalize them, while
      typechecking expressions. 

      To see this, suppose the user requests that we typecheck an expression
      of the form $(x : T)$
      where $T$ is some complicated expression entered by the user.
      Intuitively, we want to grab the type of $x$ from the context and then
      check that it is equal to the annotated type of $T$.

      Unlike the simply typed and polymorphic lambda calculi, our type system is
      much richer and so checking types for equality is not so simple.
      How do we check if 2 ``types'' are equal when they are really just expressions?
      One idea is to fully simplify them to their respective unique head normal forms,
      via normalization, and then check if those are structurally equal.

      However, it must be noted that throughout these rules, we only normalize
      expressions after we typecheck them. We always check that a type is
      well formed, ie that $\Gamma \vdash T \Rightarrow s$, before we try to
      normalize $T$.
    \end{remark}

  \item \textbf{Pi formation} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash A \Rightarrow s_1}
        \hypo{\Gamma \vdash A \Downarrow \tau}
        \hypo{(x, \, \tau, \, \text{und}) :: \Gamma \vdash B(x) \Rightarrow s_2}
        \infer3{\Gamma \vdash \Pi_{x : A}B(x) \Rightarrow s_2}
      \end{prooftree}
    \]
    Note that this is a rule schema with the metavariables 
    $s_1, \, s_2 \in \{\Type, \, \Kind\}$.

    % \[
    %   \begin{prooftree}
    %     \hypo{\Gamma \vdash A \Leftarrow \Type}
    %     \hypo{\Gamma \vdash A \Downarrow \tau}
    %     \hypo{(x, \, \tau, \, \text{undefined}) :: \Gamma \vdash B(x) \Leftarrow \Type}
    %     \infer3{\Gamma \vdash \Pi_{x : A}B(x) \Rightarrow \Type}
    %   \end{prooftree}
    % \]

  \item \textbf{Pi introduction} \\
    \[
      \begin{prooftree}
        \hypo{(x, \, \tau, \, \text{und}) :: \Gamma \vdash E \Leftarrow \tau'(x)}
        \infer1{\Gamma \vdash \lambda x, \, E \Leftarrow \Pi_{x : \tau}\tau'(x)}
      \end{prooftree}
    \]
    Note that in the event that the input variable of the lambda abstraction and
    Pi are different, then we must perform an $\alpha$ renaming so that the
    variable being bound in $(\lambda x, \, E)$ and $\Pi_{y : \tau} \tau'(y)$
    are the same.

    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash T \Rightarrow s}
        \hypo{\Gamma \vdash T \Downarrow \tau}
        \hypo{(x, \, \tau, \, \text{und}) :: \Gamma \vdash E \Rightarrow \tau'(x)}
        \infer3{\Gamma \vdash \lambda \, (x : T), \, E \Rightarrow \Pi_{x : \tau}\tau'(x)}
      \end{prooftree}
    \]
    The second rule says that if the user type annotates the input argument of
    the function, then we can try to infer the type of the output and
    consequently, the type of the function as a whole.

  \item \textbf{Function application, ie Pi elimination} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E_1 \Rightarrow \Pi_{x : \tau}\tau'(x)}
        \hypo{\Gamma \vdash E_2 \Leftarrow \tau}
        \hypo{\Gamma \vdash \tau'[x \mapsto E_2] \Downarrow \tau''}
        \infer3{E_1 \,\, E_2 \Rightarrow \tau''}
      \end{prooftree}
    \]

  \item \textbf{Local let binding} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E \Rightarrow \tau}
        \hypo{(x, \, \tau, \, \text{und}) :: \Gamma \vdash E' \Rightarrow \tau'(x)}
        \hypo{\Gamma \vdash \tau'[x \mapsto E] \Downarrow \tau''}
        \infer3{\Gamma \vdash \text{let } x := E \text{ in } E' \Rightarrow \tau''}
      \end{prooftree}
    \]

  \item \textbf{Sigma formation} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash A \Rightarrow s_1}
        \hypo{\Gamma \vdash A \Downarrow \tau}
        \hypo{(x, \, \tau, \, \text{und}) :: \Gamma \vdash B(x) \Rightarrow s_2}
        \infer3{\Gamma \vdash \Sigma_{x : A}B(x) \Rightarrow s_2}
      \end{prooftree}
    \]

    As with the rule for Pi formation, $s_1, \, s_2 \in \{\Type, \, \Kind\}$.

  \item \textbf{Sigma introduction} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E_1 \Leftarrow \tau_1}
        \hypo{\Gamma \vdash \tau_2[x \mapsto E_1] \Downarrow \tau_2'}
        \hypo{\Gamma \vdash E_2 \Leftarrow \tau_2'}
        \infer3{\Gamma \vdash \langle  E_1, \, E_2 \rangle \Leftarrow 
          \Sigma_{x : \tau_1}\tau_2(x)}
      \end{prooftree}
    \]
   
  \item \textbf{Sigma elimination} \\
    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E \Rightarrow \Sigma_{x : \tau_1}\tau_2(x)}
        \infer1{\Gamma \vdash \fst E \Rightarrow \tau_1}
      \end{prooftree}
    \]

    \[
      \begin{prooftree}
        \hypo{\Gamma \vdash E \Rightarrow \Sigma_{x : \tau_1}\tau_2(x)}
        \hypo{\Gamma \vdash \tau_2[x \mapsto \fst E] \Downarrow \tau_2'}
        \infer2{\Gamma \vdash \snd E \Rightarrow \tau_2'}
      \end{prooftree}
    \]

    % \[
    %   \begin{prooftree}
    %     \hypo{E \Rightarrow \Sigma_{x : \tau_1}\tau_2(x)}
    %     \hypo{\Gamma \vdash \tau_2[x \mapsto \fst E] \Downarrow \tau_2'}
    %     \hypo{(y, \, \tau_2', \, \text{und}) :: (x, \, \tau_1, \, \text{und}) :: \Gamma
    %           \vdash E' \rightarrow \tau}
    %     \infer3{\Gamma \vdash \text{let } (x, \, y) := E \text{ in } E' \Rightarrow \tau}
    %   \end{prooftree} 
    % \]

    % \item \textbf{Pair destructuring} \\
    % \[
    %   \begin{prooftree}
    %     \hypo{\Gamma \vdash E \Rightarrow \Sigma_{x : \tau_1}\tau_2(x)}
    %     % \hypo{(x_2, \, \tau_2[x \mapsto x_1], \text{und}) :: 
    %     %   (x_1, \, \tau_1, \, \text{und}) :: 
    %     %   \Gamma \vdash E' \Rightarrow \tau(x_1, \, x_2)}
    %     \hypo{\Gamma \vdash E' [x_1 \mapsto \fst E][x_2 \mapsto \snd E]
    %       \Rightarrow \tau}
    %     \infer2{\Gamma \vdash (\text{let } (x_1, \, x_2) := E
    %       \text{ in } E') \Rightarrow \tau}
    %   \end{prooftree}
    % \]

    \item \textbf{Sum formation} \\
    \[
      \begin{prooftree}
       \hypo{\Gamma \vdash A \Leftarrow \Type} 
       \hypo{\Gamma \vdash B \Leftarrow \Type} 
       \infer2{\Gamma \vdash A + B \Rightarrow \Type} 
      \end{prooftree}
    \]
    Note that this rule says that users can only make construct a sum, aka 
    coproduct, out of types that live in the universe \verb|Type|, not 
    \verb|Kind|.

    Recalling the Curry-Howard correspondence which identifies types and
    propositions, we see sum types as a way to model disjunction. Hence we do
    not see a need to allow users to construct sums out of large types like
    type constructors found in \verb|Kind|.

    Also we are unsure if the logical consistency of the system can be preserved
    if we allow for $A + B$ to be formed when one is of type \verb|Type| while
    the other has type \verb|Kind|. 

    \item \textbf{Sum introduction} \\
    \[
      \begin{prooftree}
       \hypo{\Gamma \vdash E \Leftarrow \tau_1} 
       \infer1{\Gamma \vdash \inl E \Leftarrow \tau_1 + \tau_2} 
      \end{prooftree}
    \]

    \[
     \begin{prooftree}
      \hypo{\Gamma \vdash E \Leftarrow \tau_2} 
      \infer1{\Gamma \vdash \inr E \Leftarrow \tau_1 + \tau_2} 
     \end{prooftree}
   \]

   \item \textbf{Sum elimination} \\
    \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Rightarrow \tau_1 + \tau_2}
      \hypo{(x, \, \tau_1, \, \text{und}) :: \Gamma \vdash E_1 \Rightarrow \tau_1'}
      \hypo{(y, \, \tau_2, \, \text{und}) :: \Gamma \vdash E_2 \Rightarrow \tau_2'}
      \hypo{\tau_1' \equiv_{\alpha \beta} \tau_2'}
      \infer4{\Gamma \vdash \match(E, \, x \rightarrow E_1, \, y \rightarrow E_2)
               \Rightarrow \tau_1'}
    \end{prooftree}  
  \]

  % \item \textbf{Local let binding} \\
  %   \[
  %     \begin{prooftree}
  %       \hypo{\Gamma \vdash E \Rightarrow \tau}
  %       \hypo{(x, \, \tau, \, \text{undefined}) :: \Gamma \vdash E' \Rightarrow \tau'(x)}
  %       \hypo{\Gamma \vdash \tau'[x \mapsto E] \Downarrow \tau''}
  %       \infer3{\Gamma \vdash (\text{let } x := E \text{ in } E') \Rightarrow \tau''}
  %     \end{prooftree}
  %   \]

  %   \[
  %     \begin{prooftree}
  %       \hypo{\Gamma \vdash (\text{let } x := (E : T)) \text{ in } E' \Rightarrow \tau}
  %       \infer1{\Gamma \vdash (\text{let } (x : T) := E \text{ in } E') \Rightarrow \tau}
  %     \end{prooftree}
  %   \]
  %   This second rule handles the case when an optional type annotation is given
  %   for the variable $x$.

  % \item \textbf{Pair destructuring} \\
  %   \[
  %     \begin{prooftree}
  %       % \hypo{\Gamma \vdash E \Rightarrow \Sigma_{x : \tau_1}\tau_2(x)}
  %       \hypo{\Gamma \vdash (\lambda z, \, E'[x_1 \mapsto \fst z][x_2 \mapsto
  %         \snd z]) \,\, E
  %         \Rightarrow \tau}
  %         \hypo{z \notin \FV(E')}
  %         \infer2{\Gamma \vdash (\text{let } \langle x_1, \, x_2 \rangle := E
  %         \text{ in } E') \Rightarrow \tau}
  %       \end{prooftree}
  %     \]

  \end{enumerate}
\end{definition}

The nice thing about these rules is that we can translate it almost directly
into a typechecking and inference algorithm!
More precisely, we may translate $\Gamma \vdash E \Rightarrow T$ into a
function called $\text{check}(\Gamma, \, E, \, T)$ which checks if the
expression $E$ really has the type $T$ given a context $\Gamma$.

Similarly, $\Gamma \vdash E \Leftarrow T$ gives us a the function
$\text{infer}(\Gamma, \, E)$ which outputs the inferred type of $E$ given
the context $\Gamma$.

In the literature, such rules are called \textit{syntax directed}, as the
algorithm closely follows the formalization of the corresponding judgments.

It's worth noting that $\eta$ equivalence is not respected by our type system.
By that we mean that the following rule does not hold:
\[
  \begin{prooftree}
    \hypo{\Gamma \vdash E \Rightarrow \tau'}
    \hypo{\tau \equiv_{\eta} \tau'}
    \infer2{\Gamma \vdash E \Leftarrow \tau}
 \end{prooftree}
\]
In other words, 2 types that are $\eta$ equivalent to one another will 
\textit{not} be judged as equal types.

One way to fix this is to allow the type checker to eta expand terms, via
eliminating and then applying the data constructor while computing the beta
normal form. However, for simplicity, we chose to follow the original 
formulation of the Calculus of Constructions and ignore this.

% Another kind of equivalence is \textit{eta equivalence}. To explain this,
% consider the terms $\lambda x, \, B \, x$ and $B$. Both of them have exactly the
% same behavior. This is one form of eta equivalence.
% In lambda calculus terms, the latter is obtained from the former by an eta
% reduction.

% Now, we may view the lambda as the data constructor for
% the Pi type and function application as the eliminator.
% Then the expression $\lambda x, \, B \, x$ is obtained from $B$ by first
% eliminating $B$ to obtain $B x$ and then using the lambda
% constructor to obtain $\lambda x, \, B \, x$.

% More generally, we say that 2 terms are eta equivalent to each other if one can
% be obtained from the other by eliminating the term and then reconstructing the
% original term using the constructor. For instance, if we had a binary coproduct,
% we could project out both components and then recombine them to obtain the
% original sum.

% Unfortunately our type system is intensional rather than extensional in
% that the following rule does \textit{not} hold: 
% \[
%   \begin{prooftree}
%     \hypo{\Gamma \vdash E \Leftarrow \tau}
%     \hypo{\tau =_\eta \tau'}
%     \infer2{\Gamma \vdash E \Leftarrow \tau'}
%   \end{prooftree}
% \]

\subsection{Big step semantics for programs}
\begin{comment}
  Follows this approach:
  https://www.cs.cornell.edu/courses/cs6110/2013sp/lectures/lec05-sp13.pdf
\end{comment}

Programs in our language are nonempty sequences of statements, with each
statement being built from some expression.
Following \href{https://web.eecs.umich.edu/~weimerw/2014-6610/reading/plotkin81structural.pdf}
{Plotkin's approach} to structural operational semantics, we formalize
the execution of program by defining a \textit{transition system}.

In the next section, we define the notion of a configuration, which plays the
same role as states in automata. Thereafter, we formalize the big-step 
transition relation for executing programs.

\subsubsection{Configuration/state of program}
A configuration has the form
\begin{align*}
  (\Gamma, \, \langle S_0; \, \dots; \, S_n \rangle, \, E)
\end{align*}
Configurations are triples representing the instantaneous state during an
execution of a program. Here, $\Gamma$ denotes the current state of the
global context and the sequence $\langle S_0; \, \dots; \, S_n \rangle$ denotes
the next statements to be executed. 
The expression $E$ is used to indicate the output of the previously executed
statement.

With this view, given a program, $\langle S_0; \, \dots; \, S_n
\rangle$, we also define:
\begin{enumerate}
\item \textbf{Initial configuration} \\
  $ (\emptyset, \, \langle S_0; \, \dots; \, S_n \rangle, \, \Type)$

  Initially, our global context is empty. Also, \verb|Type| is merely a dummy
  value and could have very well be replaced by \verb|Kind|.

\item \textbf{Final configurations} \\
  These are all configurations of the form
  $(\Gamma, \, \langle \rangle, \, E)$
\end{enumerate}

In the next section, we define the big-step transition relation for programs $\cdot
\Downarrow \cdot$ using
this notion of a configuration.

\subsubsection{Big step semantics for programs}
We define the big step transition relation, $\cdot \Downarrow \cdot$, via a
new judgment form in our calculus.

It should be noted that in the rules below, we interleave type checking and 
evaluation, ie we type check and then evaluate each statement, one at a time,
carrying the context along as we go.
We do not statically type check the whole program first, then evaluate
afterwards.

\begin{enumerate}
\item \textbf{Check} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Rightarrow s}
      \infer1{(\Gamma, \, \langle \checkk \, E \rangle, \, E') \Downarrow
        (\Gamma, \, \langle \rangle, \, \tau)}
    \end{prooftree}
  \]

\item \textbf{Eval} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Rightarrow s}
      \hypo{\Gamma \vdash E \Downarrow \nu}
      \infer2{(\Gamma, \, \langle \evall \, E \rangle, \, E') \Downarrow
        (\Gamma, \, \langle \rangle, \, \nu)}
    \end{prooftree}
  \]

\item \textbf{Axiom} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash T \Rightarrow s}
      \hypo{\Gamma \vdash T \Downarrow \tau}
      \infer2{(\Gamma, \, \langle \axiom x : T \rangle, \, E) \Downarrow
        ((x, \, \tau, \, \text{und}) :: \Gamma, \, \langle \rangle, \, x)}
    \end{prooftree}
  \]

\item \textbf{Def} \\
  \[
    \begin{prooftree}
      \hypo{\Gamma \vdash E \Rightarrow s}
      \hypo{\Gamma \vdash E \Downarrow \nu}
      \infer2{(\Gamma, \, \langle \deff x := E \rangle, \, E') \Downarrow
        ((x, \, \tau, \, \nu) :: \Gamma, \, \langle \rangle, \, x)}
    \end{prooftree}
  \]

\item \textbf{Sequences of statements} \\
  \[
    \begin{prooftree}
      \hypo{(\Gamma, \, \langle S_0 \rangle, \, E) \Downarrow
        (\Gamma', \, \langle \rangle, \, E') }
      \hypo{(\Gamma', \, \langle S_1; \, \dots; \, S_n \rangle, \, E) \Downarrow
        (\Gamma'', \, \langle \rangle, \, E'') }
      \infer2{(\Gamma, \, \langle S_0; \, S_1; \, \dots; \, S_n \rangle, \, E) \Downarrow
        (\Gamma'', \, \langle \rangle, \, E'') }
    \end{prooftree}
  \]

\end{enumerate}

\subsubsection{Putting the transition system together}
Letting $S$ denote the (infinite) set of all configurations, and defining
\[ F := \{ (\Gamma, \, \langle \rangle, \, E) \in S \, | \, E \text{ expression} \} \]

we obtain a transition system, given by the tuple
\begin{align*}
  \langle S, \, \Downarrow, \, 
  (\emptyset, \, \langle S_0; \, \dots; \, S_n \rangle, \, \Type), \, F \rangle
\end{align*}

Notice how our formalization mimics the definition of an automaton.

\subsubsection{Output expression of evaluating a program}
With these rules, given a user-entered program, say $\langle S_0;
\dots; S_n \rangle$, we define the
output expression of a program to be the $E$ such that
\begin{align*}
  (\emptyset, \, \langle S_0; \, \dots \; S_n \rangle, \, \Type) \Downarrow
  (\Gamma, \, \langle \rangle, \, E)
\end{align*}

In other words, the expression output to the user is the expression obtained by
beginning with the initial configuration and then recursively evaluating
until we reach a final configuration.

\section{Metatheoretic discussion}
The original Calculus of Constructions is known to be strongly normalizing in
that the normalization process, when applied to any well typed term, always
terminates. Furthermore, it has decidable typechecking and is logically
consistent when its type system is viewed as a logical calculus, with the Pi type
corresponding to universal quantification.

\textbf{TODO}

Say something about why we think our bidirectional typechecking works.
https://arxiv.org/pdf/2102.06513.pdf

Also say something about how ``impredicative'' Sigma types breaks consistency
due to Girard's paradox and that the only (?) way to fix that is to use a 
predicative hierarchy of type universe with either cumulativitiy or universe
polymorphism.

Mention that this issue may not be a big one because the proof term in Girard's
is astronomical and Idk if our system can even handle anything that big since it
isn't very efficient (cos we substitute naively rather than use normalization
by evaluation).

If users are really afraid, just avoid using higher order existential
quantification and stick to first order. Alternatively, can encode higher order
existential quantification in terms of universal in CoC (see Type Theory and
Formal Proof book).

https://era.ed.ac.uk/bitstream/handle/1842/12487/Luo1990.Pdf

\section{Technical implementation}

\end{document}